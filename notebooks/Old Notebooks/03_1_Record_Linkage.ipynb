{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, and Jonathan Morgan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Linkage\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning Objectives](#Learning-Objectives)\n",
    "    - [Methods](#Methods)\n",
    "- [The Principles of Record Linkage](#The-Principles-of-Record-Linkage)\n",
    "- [Data Description](#Data-Description)\n",
    "- [Python Setup](#Python-Setup)\n",
    "- [Load the Data](#Load-the-Data)\n",
    "- [Data Exploration](#Data-Exploration)\n",
    "- [Record Linkage on Business Names](#Record-Linkage-on-Business-Names)\n",
    "    - [The Importance of Pre-Processing](#The-Importance-of-Pre-Processing)\n",
    "    - [Cleaning String Variables](#Cleaning-String-Variables)\n",
    "    - [Regular Expressions – `regex`](#Regular-Expressions-–-regex)\n",
    "    - [Handling Business Suffixes](#Handling-Business-Suffixes)\n",
    "    - [Record Linkage: Exact Matching on One Field](#Record-Linkage:-Exact-Matching-on-One-Field)\n",
    "- [Record Linkage on Addresses](#Record-Linkage-on-Addresses)\n",
    "    - [Address Parsing](#Address-Parsing)\n",
    "    - [Record Linkage: Exact Matching on Several Fields](#Record-Linkage:-Exact-Matching-on-Several-Fields)\n",
    "    - [Record Linkage: Rule-Based Matching](#Record-Linkage:-Rule-Based-Matching)\n",
    "    - [Record Linkage: Fellegi Sunter](#Record-Linkage:-Fellegi-Sunter)\n",
    "- [Additional Resources](#Additional-Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This notebook will provide you with an instruction into Record Linkage using Python. Upon completion of this notebook you will be able to apply record linkage techniques using the `recordlinkage` package to combine data from different sources in Python. \n",
    "It will lead you through all the steps necessary for a successful record linkage starting with data preparation  including pre-processing, cleaning and standardization of data.\n",
    "The notebook follows the underlying lecture and provides examples on how to implement record linkage techniques. \n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "The goal of this notebook is for you to understand the record linkage techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principles of Record Linkage\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The goal of record linkage is to determine if pairs of records describe the same identity. For instance, this is important for removing duplicates from a data source or joining two separate data sources together. Record linkage also goes by the terms data matching, merge/purge, duplication detection, de-duping, reference matching, entity resolution, disambiguation, co-reference/anaphora in various fields.\n",
    "\n",
    "There are several approaches to record linkage that include \n",
    "    - exact matching \n",
    "    - rule-based linking \n",
    "    - probabilistic linking \n",
    "- An example of **exact matching** is joining records based on social security number, exact name, or geographic code information. This is what you already have done in SQL by joining tables on an unique identifier. \n",
    "- **Rule-based matching** involves applying a cascading set of rules that reflect the domain knowledge of the records being linked. \n",
    "- In **probabilistic record linkages**, linkage weights are estimated to calculate the probability of a certain match.\n",
    "\n",
    "In practical applications you will need record linkage techiques to combine information addressing the same entity that is stored in different data sources. Record linkage will also help you to address the quality of different data sources. For example, if one of your databases has missing values you might be able to fill those by finding an identical pair in a different data source. Overall, the main applications of record linkage are\n",
    "    1. Merging two or more data files \n",
    "    2. Identifying the intersection of the two data sets \n",
    "    3. Updating data files (with the data row of the other data files) and imputing missing data\n",
    "    4. Entity disambiguation and de-duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Approach\n",
    "\n",
    "For this notebook exercise we are interested in data on establishments.\n",
    "- **Analytical Exercise**: Compare business names across 2005 & 2015 employer data- and finding record pairs. \n",
    "- **Data Availability**: We have 'names', 'addresses' and 'setup_date' for the various businesses in the two time periods. \n",
    "\n",
    "- **Approach**: We will look at the data available to us- and clean & pre-process it to enable better linkage. Since the only identifier we have for this case study are the names of the firms- we will have to use string matching techniques which is enabled by record linkage package in Python. \n",
    "\n",
    "- *Caveat*: The data we use for this exercise has been taken from the QCEW data from Illinois Department of Employment Security database. As some of you might know, we do have unique identifiers (EIN) in this database. However, we have removed those identifiers for this exercise. Our dataset is also a small sample of the overall data available to reduce runtime & CUP usage in class. \n",
    "The goal of this notebook is for you to understand the recoed linkage techniques- and the analytical results at the end of this exercise should be noted as taken from a subset, and hence might not hold true for the larger database. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The dataset used in this exercise comes from the IDES (Illinois Department of Employment Security Database). \n",
    "We use the QCEW data or the Quarterly Census of Employment & Wages for our exercise. Below are some more concrete details about the dataset used: \n",
    "\n",
    "Time Period: 2005 Q1, and 2015 Q1\n",
    "\n",
    "**Variables Used**:\n",
    "- Name_legal (legal name of the firm)\n",
    "- Address (concatenated the different address strings such as house number, street address, etc to have one variable)\n",
    "- Setup Date: concatenated the day, month & time to get one date variable which refers to the setup of the establishment in question. \n",
    "\n",
    "**Filter Applied**: for the purposes of this exercise, we also removed certain records with the following criterion: \n",
    "- All records which have a unique combination of ein & name_legal... This means any record for which one ein is linked to only one name_legal is not a part of this dataset.\n",
    "- All records which have a null value for address or setup_date\n",
    "- For 2005 data in particular, for some records, we have character values (A-Z) in the variable holding #employees. We have removed such records. \n",
    "\n",
    "In a last step, we drew a sample which ensures a certain degree of deterministic name matching between the two datasets for 2005 & 2015, to make sure we have enough matches in the data for this exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Python provides us with some tools we can use for record linkages so we don't have to start from scratch and code our own linkage algorithms. Before we start we need to load the package `recordlinkage`. To fully function this packages uses other packages which also need to be imported. We are adding a couple more packages to the ones you are already familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general use imports\n",
    "%pylab inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# record linkage package\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.standardise import clean\n",
    "\n",
    "# database interaction imports\n",
    "import psycopg2\n",
    "\n",
    "print( \"Imports loaded at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "For our case study we already prepared some data for you. In order to demostrate record linkage we need directly identifiable data. In typical applications you would link based on names or some account numbers. We can't do this in class because the data in the ADRF is de-identified to guarantee data privacy for individuals who are in the data. Thus we need some other kind of information which can be linked. The QCEW (Quarterly Census of Employment and Wages) files we have for Illinois however contain names of employer. Thus, we prepared a subsample of the QCEW data for the years 2005 & 2015-- consisting of legal name, physical address, and some other establishment characteristics (see [Data Description](#Data-Description)). The data is accessible in the class database.\n",
    "\n",
    "Let's first set up our database scheme and connnect to the Database using `psycopg2`, as you've done this in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection properties\n",
    "db_host = \"10.10.2.10\"\n",
    "db_port = -1\n",
    "db_username = None\n",
    "db_password = None\n",
    "db_name = \"appliedda\"\n",
    "schema = \"ada_tanf\"\n",
    "\n",
    "# Create psycopg2 connection to Postgresql\n",
    "pgsql_connection = psycopg2.connect( host = db_host, database = db_name )\n",
    "\n",
    "# Read SQL \n",
    "sql_string2015 = \"select distinct name_legal as orig_name, address, setup_date, total_wages, tot_empl from \" + schema + \".\" + \"rec_link_2015\"\n",
    "sql_string2005 = \"select distinct name_legal as orig_name, address, setup_date, total_wages, tot_empl from \" + schema + \".\" + \"rec_link_2005\"\n",
    "\n",
    "# Save table in dataframe\n",
    "d_2015 = pd.read_sql(sql_string2015, con=pgsql_connection)\n",
    "d_2005 = pd.read_sql(sql_string2005, con=pgsql_connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Exploration\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Next, we want to get to know the data a bit so we need to know what kind of pre-processing we have to apply. What you want to check for example are formats, missing values, and the quality of your data in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Visualizing the top rows of the different datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2005.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Shape and Properties__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the data frame\n",
    "print(d_2005.shape)\n",
    "print(d_2015.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2005.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2015.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Checking for NULLS   \n",
    "d_2015['address'].isnull().values.any()\n",
    "# Output: boolean operator telling if there are any null values or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2015['address'].isnull().sum()\n",
    "# Output: count of the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2015.isnull().sum()\n",
    "# Output: count of null values for all of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Checking for specific values such as '.', '', etc. \n",
    "d_2015['address'].isin(['', '.']).values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2015['address'].isin(['', '.']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Pre-Processing\n",
    "Data pre-processing is an important step in a data anlysis project in general, in record linkage applications in particular. The goal of pre-processing is to transform messy data into a dataset that can be used in a project workflow.\n",
    "\n",
    "Linking records from different data sources comes with different challenges that need to be addressed by the analyst. The analyst must determine whether or not two entities (individuals, businesses, geographical units) on two different files are the same. This determination is not always easy. In most of the cases there is no common uniquely identifing characteristic for a entity. For example, is Bob Miller from New Yor the same person as Bob Miller from Chicago in a given dataset? This detemination has to be executed carefully because consequences of wrong linkages may be substantial (is person X the same person as the person X on the list of identified terrorists). Pre-processing can help to make better informed decisions.\n",
    "\n",
    "Pre-processing can be difficult because there are a lot of things to keep in mind. For example, data input errors, such as typos, misspellings, truncation, abbreviations, and missing values need to be corrected. Literature shows that preprocessing can improve matches. In some situations, 90% of the improvement in matching efficiency may be due to preprocessing. The most common reason why matching projects fail is lack of time and resources for data cleaning. \n",
    "\n",
    "In the following we will walk you through some pre-processing steps, these include but are not limited to removing spaces, parsing fields, and standardizing strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the the most recurring business names in the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d_2005['orig_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2015['orig_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Right away, we notice that the record linkage between the different datasets will not be straightforward. The variable is messy and non-standardized, similar names can be written differently (in upper-case or lower-case characters, with or without suffixes, etc.) The essential next step is to process the variables in order to make the linkage the most effective and relevant possible.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parsing String Variables\n",
    "\n",
    "By default, the split method returns a list of strings obtained by splitting the original string on spaces or commas, etc. The record linkage package comes with a build in cleaning function we can also use. In addition, we can extract information from strings for example by using regex search commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uppercasing names and creating a new column of names to work with\n",
    "d_2015['orig_name_clean']=d_2015.orig_name.str.upper()\n",
    "\n",
    "# Cleaning names (using the record linkage package tool, see imports)\n",
    "# Clean removes any characters such as '-', '.', '/', '\\', ':', brackets of all types. \n",
    "d_2015['orig_name_clean']=clean(d_2015['orig_name_clean'], lowercase=False, strip_accents='ascii', \\\n",
    "                                remove_brackets=False)\n",
    "d_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regular Expressions – `regex`\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "Regular expressions (regex) are a way of searching for a character pattern. They can be used for matching or replacing operations in strings.\n",
    "\n",
    "When defining a regular expression search pattern, it is a good idea to start out by writing down, explicitly, in plain English, what you are trying to search for and exactly how you identify when you've found a match.\n",
    "For example, if we look at an author field formatted as \"&lt;last_name&gt; , &lt;first_name&gt; &lt;middle_name&gt;\", in plain English, this is how I would explain where to find the last name: \"starting from the beginning of the line, take all the characters until you see a comma.\"\n",
    "\n",
    "\n",
    "In a regular expression, there are special reserved characters and character classes. For example:\n",
    "- \"`^`\" matches the beginning of the line or cell\n",
    "- \"`.`\" matches any character\n",
    "- \"`+`\" means one or more repetitions of the preceding expressions\n",
    "\n",
    "Anything that is not a special charater or class is just looked for explicitly. A comma, for example, is not a special character in regular expressions, so inserting \"`,`\" in a regular expression will simply match that character in the string.\n",
    "\n",
    "In our example, in order to extract the last name, the resulting regular expression would be:\n",
    "\"`^.+,`\". We start at the beginning of the line ( \"`^`\" ), matching any characters ( \"`.+`\" ) until we come to the literal character of a comma ( \"`,`\" ).\n",
    "\n",
    "\n",
    "_Note: if you want to actually look for one of these reserved characters, it must be escaped, so that, for example, the expression looks for a literal period, rather than the special regular expression meaning of a period. To escape a reserved character in a regular expression, precede it with a back slash ( \"`\\`\" ). For example, \"`\\.`\" will match a \"`.`\" character in a string._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__REGEX CHEATSHEET__\n",
    "\n",
    "\n",
    "    - abc...     Letters\n",
    "    - 123...     Digits\n",
    "    - \\d         Any Digit\n",
    "    - \\D         Any non-Digit Character\n",
    "    - .          Any Character\n",
    "    - \\.         Period\n",
    "    - [a,b,c]    Only a, b or c\n",
    "    - [^a,b,c]   Not a,b, or c\n",
    "    - [a-z]      Characters a to z\n",
    "    - [0-9]      Numbers 0 to 9\n",
    "    - \\w any     Alphanumeric chracter\n",
    "    - \\W         any non-Alphanumeric character\n",
    "    - {m}        m Repetitions\n",
    "    - {m,n}      m to n repetitions\n",
    "    - *          Zero or more repetitions\n",
    "    - +          One or more repetitions\n",
    "    - ?          Optional Character\n",
    "    - \\s         any Whitespace\n",
    "    - \\S         any non-Whitespace character\n",
    "    - ^...$      Starts & Ends\n",
    "    - (...)      Capture Group\n",
    "    - (a(bc))    Capture sub-Group\n",
    "    - (.*)       Capture All\n",
    "    - (abc|def)  Capture abc or def\n",
    "\n",
    "__Examples:__\n",
    "    - `(\\d\\d|\\D)`      will match 22X, 23G, 56H, etc...\n",
    "    - `(\\w)`           will match any characters between 0-9 or a-z\n",
    "    - `(\\w{1-3})`      will match any alphanumeric character of a length of 1 to 3. \n",
    "    - `(spell|spells)` will match spell or spells\n",
    "    - `(corpo?)        will match corp or corpo\n",
    "    - `(feb 2.)`       will match feb 20, feb 21, feb 2a, etc.\n",
    "\n",
    "\n",
    "__Using REGEX to match characters:__\n",
    "\n",
    "In python, to use a regular expression like this to search for matches in a given string, we use the built-in \"`re`\" package ( https://docs.python.org/2/library/re.html ), specifically the \"`re.search()`\" method. To use \"`re.search()`\", pass it first the regular expression you want to use to search, enclosed in quotation marks, and then the string you want to search within. \n",
    "\n",
    "\n",
    "\n",
    "__Using REGEX for replacing characters:__\n",
    "\n",
    "The `re` package also has an \"`re.sub()`\" method used to replace regular expressions by other strings. The method can be applied to an entire pandas column (replacing expression1 with expression2) with the following syntax: `df['variable'].str.replace(r'expression1', 'expression2')`. Note the `r` before the first string to signal we are using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting ZIPCODE\n",
    "# Since a US Zipcode typically follows a set pattern of 5 digits or 5digits followed by a hypen and then 4 digits\n",
    "# We can use this information to extract zipcodes. \n",
    "\n",
    "# Pattern1\n",
    "d_2015['zipcode']=d_2015['address'].str.extract('(\\d{5})')\n",
    "\n",
    "# Breaking the code down: \n",
    "# \\d ---- tells that we need a digit\n",
    "# \\d{5} ---- tells that we need 5 digits consecutively\n",
    "# () enclosing brackets tell that we need to extract this information in the new variable\n",
    "d_2015[['zipcode', 'address','setup_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2\n",
    "# What if we want the full zipcode: '5 digits <hyphen> 4 digits'\n",
    "d_2015['zipcode_full']=d_2015['address'].str.extract('(\\d{5}-\\d{4})')\n",
    "\n",
    "# Breaking the code down: \n",
    "# \\d ---- tells that we need a digit\n",
    "# \\d{5} ---- tells that we need 5 digits consecutively\n",
    "# '-' this is just passing the string exactly as we need it\n",
    "# \\d{4} tells us 4 more digits\n",
    "# () enclosing brackets tell that we need to extract this information in the new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3\n",
    "# We can also pass both the expressions in our query as an OR.\n",
    "d_2015['zipcode_either']=d_2015['address'].str.extract('(\\d{5}-\\d{4}|\\d{5})')\n",
    "\n",
    "# Breaking the code: \n",
    "# \\d ---- tells that we need a digit\n",
    "# \\d{5} ---- tells that we need 5 digits consecutively\n",
    "# '-' this is just passing the string exactly as we need it\n",
    "# \\d{4} tells us 4 more digits\n",
    "# () enclosing brackets tell that we need to extract this information in the new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the date into day, month & year\n",
    "\n",
    "d_2015['day']=d_2015['setup_date'].str.extract('(\\A\\d{1,2})', expand=True)\n",
    "# d{1,2} is looking for a 2 digit variable\n",
    "# \\A is indicating that we want this 2 digit variable to be at the start of the string\n",
    "\n",
    "d_2015['month']=d_2015['setup_date'].str.extract('\\d{1,2}.(\\d{1,2})', expand=True)\n",
    "# (\\d{1,2}) is enclosed in brackets--- and shows that this is the only part we really want to be extracted\n",
    "# \\d\\d.  -- the 2 digit long enclosure is preceded by /d{1,2} \n",
    "###### indicating that the pattern is preceded by a 1 or 2 digits followed by a .\n",
    "\n",
    "d_2015['year']=d_2015['setup_date'].str.extract('\\w+.(\\d{4})', expand=True)\n",
    "# Here, \\w+ indicates that we need 4 digits which are preceded by an alphanumeric character string of however long length\n",
    "#### and that should be followed by a '.'\n",
    "\n",
    "d_2015[['setup_date', 'day', 'month', 'year']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Business Suffixes\n",
    "\n",
    "You may have noticed that several business names finish with a legal suffix (\"CO\", \"INC\", \"LIMITED LIABILITY\", etc.). Unfortunately, examples below will show that these suffixes are not consistent between tables, and they make record linkage impossible. Below we detail one possible way of dealing with legal suffixes. \n",
    "\n",
    "We will start by getting the legal suffix of the businesses. We will then isolate them into a separate variable, then standardize them. When matching the dataframes, we can now choose to match on both business name and legal suffixes, or on business name stripped of the legal suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the last character in the cleaned name variable\n",
    "d_2015['lname'] = d_2015.orig_name_clean.str.split(' ').str.get(-1)\n",
    "\n",
    "# Creating a new variable 'legal_type' and assigning it value based on the values in the last character of name variable. \n",
    "# If the last character in the name variable does not belong to the pre-set list of legal types- we assign a null value to this character\n",
    "legal=pd.Series(['INC', 'LTD','LIMITED', 'INCORPORATED', 'INCORPORATION', \\\n",
    "                 'ASSOCIATION','CORP', 'CORPORATION' 'CO', 'LLC', 'ASSOC', 'ASSOCIATES', 'PTNRSHP', 'COMPANY'])\n",
    "\n",
    "# Creating an indicator which tells us if the last name is specifying the legal name of the establishment or not\n",
    "d_2015['ind']=d_2015['lname'].isin(legal)\n",
    "d_2015.head()\n",
    "\n",
    "# Getting legal names\n",
    "d_2015['legal_name']=np.where(d_2015['ind']==1, d_2015['lname'], None)\n",
    "\n",
    "d_2015['len']=d_2015.orig_name_clean.str.len()-d_2015.lname.str.len()\n",
    "d_2015['name_2'] = d_2015.apply(lambda r: r.orig_name_clean[:r.len], axis=1)\n",
    "\n",
    "# Here, axis=1 says we are performing this function over the column\n",
    "# Apply tells that we are performing this function one row at a time\n",
    "# Lambda is a set of methods in pandas \n",
    "\n",
    "d_2015['est_name']=np.where(d_2015['ind']==1, d_2015['name_2'], d_2015['orig_name_clean'])\n",
    "d_2015[['orig_name_clean','est_name', 'legal_name', 'lname','ind', 'len','name_2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the legal names\n",
    "# ['INC', 'LTD', 'LIMITED', 'INCORPORATED', 'INCORPORATION', 'ASSOCIATION','CORP', 'CORPORATION' 'CO', \n",
    "# LLC', 'ASSOC', 'ASSOCIATES', 'PTNRSHP', 'COMPANY'])\n",
    "\n",
    "conditions= [\n",
    "    (d_2015['legal_name']=='INC'), \n",
    "    (d_2015['legal_name']=='LTD'), \n",
    "    (d_2015['legal_name']=='LIMITED'), \n",
    "    (d_2015['legal_name']=='INCORPORATED'), \n",
    "    (d_2015['legal_name']=='INCORPORATION'), \n",
    "    (d_2015['legal_name']=='ASSOCIATION'), \n",
    "    (d_2015['legal_name']=='CORP'), \n",
    "    (d_2015['legal_name']=='CORPORATION'),\n",
    "    (d_2015['legal_name']=='CO'),\n",
    "    (d_2015['legal_name']=='LLC'), \n",
    "    (d_2015['legal_name']=='ASSOC'), \n",
    "    (d_2015['legal_name']=='ASSOCIATES'), \n",
    "    (d_2015['legal_name']=='PTNRSHP') ,\n",
    "    (d_2015['legal_name']=='COMPANY')\n",
    "]\n",
    "\n",
    "choices=['INC', 'LTD', 'LTD', 'INC', 'INC', 'ASSOCIATION', 'CORP', 'CORP', \\\n",
    "         'CO', 'LLC', 'ASSOCIATION', 'ASSOCIATES', 'PARTNERSHIP', 'COMPANY']\n",
    "\n",
    "d_2015['legal_name_clean']=np.select(conditions, choices, default=None)\n",
    "\n",
    "# This is how our prepared data looks\n",
    "d_2015[['est_name', 'legal_name_clean', 'address', 'setup_date']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any other possible standardizations? Insert them below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with the inital data prep work. Please keep in mind that we just provided some examples for you to demonstrate the process. You can add as many further steps to it as necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will just run the same code on the year 2005 data. You can also run this in a loop over both years\n",
    "d_2005['orig_name_clean']=d_2005.orig_name.str.upper()\n",
    "d_2005['orig_name_clean']=clean(d_2005['orig_name_clean'], lowercase=False, strip_accents='ascii', \\\n",
    "                                remove_brackets=False)\n",
    "d_2005['lname'] = d_2005.orig_name_clean.str.split(' ').str.get(-1)\n",
    "legal=pd.Series(['INC', 'LTD', 'COMPANY', 'CORPORATION', 'LIMITED', 'INCORPORATED', 'INCORPORATION', \\\n",
    "                 'ASSOCIATION','CORP', 'LLC', 'ASSOC', 'ASSOCIATES', 'PTNRSHP'])\n",
    "d_2005['ind']=d_2005['lname'].isin(legal)\n",
    "d_2005['legal_name']=np.where(d_2005['ind']==1, d_2005['lname'], None)\n",
    "d_2005['len']=d_2005.orig_name_clean.str.len()-d_2005.lname.str.len()\n",
    "d_2005['name_2'] = d_2005.apply(lambda r: r.orig_name_clean[:r.len], axis=1)\n",
    "d_2005['est_name']=np.where(d_2005['ind']==1, d_2005['name_2'], d_2005['orig_name_clean'])\n",
    "conditions= [\n",
    "    (d_2005['legal_name']=='INC'), \n",
    "    (d_2005['legal_name']=='LTD'), \n",
    "    (d_2005['legal_name']=='LIMITED'), \n",
    "    (d_2005['legal_name']=='INCORPORATED'), \n",
    "    (d_2005['legal_name']=='INCORPORATION'), \n",
    "    (d_2005['legal_name']=='ASSOCIATION'), \n",
    "    (d_2005['legal_name']=='CORP'), \n",
    "    (d_2005['legal_name']=='CORPORATION'),\n",
    "    (d_2005['legal_name']=='CO'),\n",
    "    (d_2005['legal_name']=='LLC'), \n",
    "    (d_2005['legal_name']=='ASSOC'), \n",
    "    (d_2005['legal_name']=='ASSOCIATES'), \n",
    "    (d_2005['legal_name']=='PTNRSHP') ,\n",
    "    (d_2005['legal_name']=='COMPANY')\n",
    "]\n",
    "choices=['INC', 'LTD', 'LTD', 'INC', 'INC', 'ASSOCIATION', 'CORP', 'CORP', 'CO', 'LLC', \\\n",
    "         'ASSOCIATION', 'ASSOCIATES', 'PARTNERSHIP', 'COMPANY']\n",
    "d_2005['legal_name_clean']=np.select(conditions, choices, default=None)\n",
    "d_2005['zipcode']=d_2005['address'].str.extract('(\\d{5})')\n",
    "d_2005['day']=d_2005['setup_date'].str.extract('(\\A\\d{1,2})', expand=True)\n",
    "d_2005['month']=d_2005['setup_date'].str.extract('\\d{1,2}.(\\d{1,2})', expand=True)\n",
    "d_2005['year']=d_2005['setup_date'].str.extract('\\w+.(\\d{4})', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linkage\n",
    "The record linkage package is a quite powerful tool for you to use when you want to link records within a dataset or across multiple datasets. It comes with different bulid in distances metrics and comparison functions, however, it also allows you to create your own. In general record linkage is divided in several steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the match later so we have different names for years\n",
    "d_2015.rename(columns={'est_name':'est_name_2015'}, inplace=True)\n",
    "d_2005.rename(columns={'est_name':'est_name_2005'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep variables relevant for linkage\n",
    "d_2015 = d_2015[['est_name_2015', 'zipcode', 'year', 'legal_name_clean']]\n",
    "d_2005 = d_2005[['est_name_2005', 'zipcode', 'year', 'legal_name_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2015.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2005.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already done the pre-processing, so the next step is indexing the data we would like to link. Indexing allows you to create candidate links, which basically means identifying pairs of data rows which might refer to the same real world entity. This is also called the comparison space (matrix). There are different ways to index data. The easiest is to create a full index and consider every pair a match. This is also the least efficient method, because we will be comparing every row of one dataset with every row of the other dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a full index first (comparison table of all possible linkage combinations)\n",
    "indexer = rl.FullIndex()\n",
    "pairs = indexer.index(d_2015, d_2005)\n",
    "# Returns a pandas MultiIndex object\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do better if we actually include our knowledge about the data to eliminate bad link from the start. This can be done through blocking. The recordlinkage packages gives you multiple options for this. For example, you can block by using variables, which menas only links exactly equal on specified values will be kept. You can also use a neighbourhood index in which the rows in your dataframe are ranked by some value and python will only link between the rows that are closeby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexerBL = rl.BlockIndex(on='zipcode')\n",
    "pairs2 = indexerBL.index(d_2015, d_2005)\n",
    "# Returns a pandas MultiIndex object\n",
    "print(len(pairs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate compare object (we are using the blocked ones here)\n",
    "# You want to give python the name of the MultiIndex and the names of the datasets\n",
    "compare = rl.Compare(pairs2, d_2015,d_2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set up our comparison space. We can start to compareour files and see if we find matches. We will demonstrate an exact match and rule based approches using distance measures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact comparison\n",
    "# This compares all the pairs of strings for exact matches \n",
    "# It is similar to a JOIN-- \n",
    "exact = compare.exact('est_name_2015','est_name_2005',name='exact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command gives us the probability of match between two strings basis the levenshtein distance\n",
    "# The measure is 0 if there are no similarities in thee string, 1 if it's identical  \n",
    "levenshtein = compare.string('est_name_2015','est_name_2005', name='levenshtein')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command gives us the probability of match between two strings basis the jarowinkler distance\n",
    "# The measure is 0 if there are no similarities in thee string, 1 if it's identical \n",
    "jarowinkler_name = compare.string('est_name_2015','est_name_2005', method='jarowinkler', name='jarowinkler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally- we can compare the different metrics for an aggregate comparison of their performance \n",
    "print(compare.vectors.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Once we have our comparison measures we need to classify the measure in matches and non matches. A rule based approach would be to say if the similarity of our indicators is 0.70 or higher we consider this a match, everything else we won't match. This decision need to be made by the analyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify matches \n",
    "matches = compare.vectors[compare.vectors.max(axis=1) > 0.80]\n",
    "matches = matches.sort_values(\"jarowinkler\")\n",
    "matches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the list of matches we can fuse our dataset, becasue at the end we want to have a combined dataset. We are using a function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse(dfA, dfB, dfmatches):\n",
    "    newDF = dfA.copy()\n",
    "    columns = dfB.columns.values\n",
    "    \n",
    "    for col in columns:\n",
    "        newDF[col] = newDF.apply(lambda _: '', axis=1)\n",
    "        \n",
    "    for row in dfmatches.iterrows():\n",
    "        indexA = row[0][0]\n",
    "        indexB = row[0][1]\n",
    "        \n",
    "        for col in columns:\n",
    "            newDF.loc[indexA][col] = dfB.loc[indexB][col]\n",
    "    return newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fuse(d_2015, d_2005, matches)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of classiying records is the Fellegi Sunter Method. If Fellegi Sunter is used to classify record pairs you would follow all the step we have done so far. However now, we would estimate probabilities to construct weights. These weights will then be applied during the classification to give certain characteristics more importance. For example we are more certain that very unique names are a match than Bob Millers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fellegi Sunter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = compare.string('est_name_2015','est_name_2005', method='jarowinkler', name='name')\n",
    "# legal = compare.string('legal_name_clean','legal_name_clean', method='jarowinkler', name='legal')\n",
    "matches = compare.vectors[compare.vectors.max(axis=1) > 0.80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Training Data and index\n",
    "ml_pairs = matches[0:40000]\n",
    "ml_matches_index = ml_pairs.index & pairs2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Naive Bayes classifier is a probabilistic classifier. The probabilistic record linkage framework by Fellegi and Sunter (1969) is the most well-known probabilistic classification method for record linkage. Later, it was proved that the Fellegi and Sunter method is mathematically equivalent to the Naive Bayes method in case of assuming independence between comparison variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the classifier\n",
    "nb = rl.NaiveBayesClassifier()\n",
    "nb.learn(ml_pairs, ml_matches_index)\n",
    "\n",
    "## Predict the match status for all record pairs\n",
    "result_nb = nb.predict(matches)\n",
    "\n",
    "## Predict probability for record to be a match\n",
    "prob_nb = nb.prob(matches)## Check header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check header\n",
    "prob_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "The last step is to evaluate the results of the record linkage. We will cover this in more detail in the machine learning session. This is just for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix\n",
    "conf_nb = rl.confusion_matrix(ml_pairs, result_nb, len(matches))\n",
    "conf_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision and Accuracy\n",
    "precision = rl.precision(conf_nb)\n",
    "accuracy = rl.accuracy(conf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision and Accuracy\n",
    "print(precision)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The F-score for this classification is\n",
    "rl.fscore(conf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Readings\n",
    "\n",
    "### Parsing\n",
    "\n",
    "* Python online documentation: https://docs.python.org/2/library/string.html#deprecated-string-functions\n",
    "* Python 2.7 Tutorial(Splitting and Joining Strings): http://www.pitt.edu/~naraehan/python2/split_join.html\n",
    "\n",
    "### Regular Expression\n",
    "\n",
    "* Python documentation: https://docs.python.org/2/library/re.html#regular-expression-syntax\n",
    "* Online regular expression tester (good for learning): http://regex101.com/\n",
    "\n",
    "### String Comparators\n",
    "\n",
    "* GitHub page of jellyfish: https://github.com/jamesturk/jellyfish\n",
    "* Different distances that measure the differences between strings:\n",
    "    - Levenshtein distance: https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "    - Damerau–Levenshtein distance: https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\n",
    "    - Jaro–Winkler distance: https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\n",
    "    - Hamming distance: https://en.wikipedia.org/wiki/Hamming_distance\n",
    "    - Match rating approach: https://en.wikipedia.org/wiki/Match_rating_approach\n",
    "\n",
    "### Fellegi-Sunter Record Linkage \n",
    "\n",
    "* Introduction to Probabilistic Record Linkage: http://www.bristol.ac.uk/media-library/sites/cmm/migrated/documents/problinkage.pdf\n",
    "* Paper Review: https://www.cs.umd.edu/class/spring2012/cmsc828L/Papers/HerzogEtWires10.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "728px",
    "left": "0px",
    "right": "1021px",
    "top": "110px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
